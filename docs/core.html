---

title: module name here


keywords: fastai
sidebar: home_sidebar

summary: "API details."
description: "API details."
nb_path: "00_core.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 00_core.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Gabor-Layer-Math">Gabor Layer Math<a class="anchor-link" href="#Gabor-Layer-Math"> </a></h1><p>{% raw %}
$$G_\theta(x',y';\sigma,\gamma,\lambda,\psi):=e^{-\frac{(x'^2+\gamma^2y'^2)}{\sigma^2}}cos(\lambda x' + \psi)$$
{% endraw %}
{% raw %}
$$x' = x cos\theta - y sin\theta$$
{% endraw %}
{% raw %}
$$y' = x sin\theta + y cos\theta$$
{% endraw %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GaborLayer" class="doc_header"><code>class</code> <code>GaborLayer</code><a href="https://github.com/SRSteinkamp/pwc_gabor_layer/tree/main/pwc_gabor_layer/core.py#L18" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GaborLayer</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>) :: <code>Layer</code></p>
</blockquote>
<p>This is the class from which all layers inherit.</p>
<p>A layer is a callable object that takes as input one or more tensors and
that outputs one or more tensors. It involves <em>computation</em>, defined
in the <code>call()</code> method, and a <em>state</em> (weight variables), defined
either in the constructor <code>__init__()</code> or in the <code>build()</code> method.</p>
<p>Users will just instantiate a layer and then treat it as a callable.</p>
<p>We recommend that descendants of <code>Layer</code> implement the following methods:</p>
<ul>
<li><code>__init__()</code>: Defines custom layer attributes, and creates layer state
variables that do not depend on input shapes, using <code>add_weight()</code>.</li>
<li><code>build(self, input_shape)</code>: This method can be used to create weights that
depend on the shape(s) of the input(s), using <code>add_weight()</code>. <code>__call__()</code>
will automatically build the layer (if it has not been built yet) by
calling <code>build()</code>.</li>
<li><code>call(self, *args, **kwargs)</code>: Called in <code>__call__</code> after making sure
<code>build()</code> has been called. <code>call()</code> performs the logic of applying the
layer to the input tensors (which should be passed in as argument).
Two reserved keyword arguments you can optionally use in <code>call()</code> are:<ul>
<li><code>training</code> (boolean, whether the call is in
inference mode or training mode)</li>
<li><code>mask</code> (boolean tensor encoding masked timesteps in the input, used
in RNN layers)</li>
</ul>
</li>
<li><code>get_config(self)</code>: Returns a dictionary containing the configuration used
to initialize this layer. If the keys differ from the arguments
in <code>__init__</code>, then override <code>from_config(self)</code> as well.
This method is used when saving
the layer or a model that contains this layer.</li>
</ul>
<p>Examples:</p>
<p>Here's a basic example: a layer with two variables, <code>w</code> and <code>b</code>,
that returns <code>y = w . x + b</code>.
It shows how to implement <code>build()</code> and <code>call()</code>.
Variables set as attributes of a layer are tracked as weights
of the layers (in <code>layer.weights</code>).</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleDense</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">SimpleDense</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">=</span> <span class="n">units</span>

  <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>  <span class="c1"># Create the state of the layer (weights)</span>
    <span class="n">w_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
        <span class="n">initial_value</span><span class="o">=</span><span class="n">w_init</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">),</span>
                             <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">),</span>
        <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">b_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
        <span class="n">initial_value</span><span class="o">=</span><span class="n">b_init</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">),</span>
        <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>  <span class="c1"># Defines the computation from inputs to outputs</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>

<span class="c1"># Instantiates the layer.</span>
<span class="n">linear_layer</span> <span class="o">=</span> <span class="n">SimpleDense</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># This will also call `build(input_shape)` and create the weights.</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">linear_layer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>

<span class="c1"># These weights are trainable, so they&#39;re listed in `trainable_weights`:</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">trainable_weights</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
</pre></div>
<p>Note that the method <code>add_weight()</code> offers a shortcut to create weights:</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleDense</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">SimpleDense</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">=</span> <span class="n">units</span>

  <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">),</span>
                               <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;random_normal&#39;</span><span class="p">,</span>
                               <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">,),</span>
                               <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;random_normal&#39;</span><span class="p">,</span>
                               <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
</pre></div>
<p>Besides trainable weights, updated via backpropagation during training,
layers can also have non-trainable weights. These weights are meant to
be updated manually during <code>call()</code>. Here's a example layer that computes
the running sum of its inputs:</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ComputeSum</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">ComputeSum</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
      <span class="c1"># Create a non-trainable weight.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">total</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">input_dim</span><span class="p">,)),</span>
                               <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">total</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">total</span>

<span class="n">my_sum</span> <span class="o">=</span> <span class="n">ComputeSum</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">my_sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># [2. 2.]</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">my_sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># [4. 4.]</span>

<span class="k">assert</span> <span class="n">my_sum</span><span class="o">.</span><span class="n">weights</span> <span class="o">==</span> <span class="p">[</span><span class="n">my_sum</span><span class="o">.</span><span class="n">total</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">my_sum</span><span class="o">.</span><span class="n">non_trainable_weights</span> <span class="o">==</span> <span class="p">[</span><span class="n">my_sum</span><span class="o">.</span><span class="n">total</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">my_sum</span><span class="o">.</span><span class="n">trainable_weights</span> <span class="o">==</span> <span class="p">[]</span>
</pre></div>
<p>For more information about creating layers, see the guide
<a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models">Writing custom layers and models with Keras</a></p>
<p>Arguments:
  trainable: Boolean, whether the layer's variables should be trainable.
  name: String name of the layer.
  dtype: The dtype of the layer's computations and weights (default of
    <code>None</code> means use <code>tf.keras.backend.floatx</code> in TensorFlow 2, or the type
    of the first input in TensorFlow 1).
  dynamic: Set this to <code>True</code> if your layer should only be run eagerly, and
    should not be used to generate a static computation graph.
    This would be the case for a Tree-RNN or a recursive network,
    for example, or generally for any layer that manipulates tensors
    using Python control flow. If <code>False</code>, we assume that the layer can
    safely be used to generate a static computation graph.</p>
<p>Attributes:
  name: The name of the layer (string).
  dtype: The dtype of the layer's computations and weights. If mixed
    precision is used with a <code>tf.keras.mixed_precision.experimental.Policy</code>,
    this is instead just the dtype of the layer's weights, as the computations
    are done in a different dtype.
  updates: List of update ops of this layer.
  losses: List of losses added by this layer.
  trainable_weights: List of variables to be included in backprop.
  non_trainable_weights: List of variables that should not be
    included in backprop.
  weights: The concatenation of the lists trainable_weights and
    non_trainable_weights (in this order).
  trainable: Whether the layer should be trained (boolean).
  input_spec: Optional (list of) <code>InputSpec</code> object(s) specifying the
    constraints on inputs that can be accepted by the layer.</p>
<p>Each layer has a dtype, which is typically the dtype of the layer's
computations and variables. A layer's dtype can be queried via the
<code>Layer.dtype</code> property. The dtype is specified with the <code>dtype</code> constructor
argument. In TensorFlow 2, the dtype defaults to <code>tf.keras.backend.floatx()</code>
if no dtype is passed. <code>floatx()</code> itself defaults to "float32". Additionally,
layers will cast their inputs to the layer's dtype in TensorFlow 2. When mixed
precision is used, layers may have different computation and variable dtypes.
See <code>tf.keras.mixed_precision.experimental.Policy</code> for details on layer
dtypes.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaborLayer.create_xy_grid" class="doc_header"><code>GaborLayer.create_xy_grid</code><a href="https://github.com/SRSteinkamp/pwc_gabor_layer/tree/main/pwc_gabor_layer/core.py#L36" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaborLayer.create_xy_grid</code>()</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Test the outputs size of the meshgrid.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">GaborLayer</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">orientations</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">create_xy_grid</span><span class="p">()</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>

<span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">GaborLayer</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">orientations</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">create_xy_grid</span><span class="p">()</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaborLayer.build" class="doc_header"><code>GaborLayer.build</code><a href="https://github.com/SRSteinkamp/pwc_gabor_layer/tree/main/pwc_gabor_layer/core.py#L47" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaborLayer.build</code>(<strong><code>batch_input_shape</code></strong>)</p>
</blockquote>
<p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <code>Layer</code> or <code>Model</code>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <code>Layer</code> subclasses.</p>
<p>Arguments:
  input_shape: Instance of <code>TensorShape</code>, or list of instances of
    <code>TensorShape</code> if the layer expects a list of inputs
    (one instance per input).</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaborLayer.create_kernel" class="doc_header"><code>GaborLayer.create_kernel</code><a href="https://github.com/SRSteinkamp/pwc_gabor_layer/tree/main/pwc_gabor_layer/core.py#L91" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaborLayer.create_kernel</code>()</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">GL</span> <span class="o">=</span> <span class="n">GaborLayer</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">),</span> <span class="n">orientations</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">GL</span><span class="o">.</span><span class="n">build</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

